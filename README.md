# ETL Pipeline con Apache Spark en Google Colab

## üìå Descripci√≥n del Proyecto
Este proyecto implementa un **ETL (Extract, Transform, Load) Pipeline** utilizando **Apache Spark** en **Google Colab**. Su objetivo es procesar datos de reservas hoteleras y generar insights clave mediante transformaciones y an√°lisis exploratorio de datos (EDA).

## üöÄ Tecnolog√≠as Utilizadas
- **Apache Spark** (PySpark)
- **Google Colab**
- **Python** (pandas, matplotlib)
- **Google Drive** (para almacenamiento de archivos)

## ‚öôÔ∏è Requerimientos T√©cnicos
### üìå Hardware Necesario
- Procesador: M√≠nimo **Intel Core i5** o equivalente
- Memoria RAM: **8GB** (recomendado 16GB o m√°s)
- Espacio en Disco: **Al menos 10GB** libres

### üìå Instalaciones Preliminares
- **Google Colab** (se ejecuta en la nube, no requiere instalaci√≥n local)
- **Google Drive** (para almacenamiento de archivos de entrada/salida)
- **Apache Spark** (instalaci√≥n dentro de Colab, se incluye en el c√≥digo)

### üìå Dependencias
Las siguientes librer√≠as deben instalarse en el entorno de ejecuci√≥n:
```bash
!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz
!tar xf spark-3.5.4-bin-hadoop3.tgz
!pip install -q findspark
```

## üìÇ Estructura del Proyecto
```
Proyecto_Booking_Raul_AlvaroProleon/
‚îú‚îÄ‚îÄ input/                                     # Archivos de entrada
‚îÇ   ‚îú‚îÄ‚îÄ hotel_bookings.csv                     # Dataset de reservas hoteleras
‚îú‚îÄ‚îÄ output/                                    # Archivos de salida
‚îÇ   ‚îú‚îÄ‚îÄ tabla_bkg_reservas.csv                 # Reservas y cancelaciones por mes y a√±o
‚îÇ   ‚îú‚îÄ‚îÄ tabla_bkg_dias_espera.csv              # Promedio de d√≠as de espera y hu√©spedes
‚îÇ   ‚îú‚îÄ‚îÄ tabla_bkg_tarifa.csv                   # Tarifas diarias promedio por habitaci√≥n
‚îú‚îÄ‚îÄ src/                                       # Archivos de entrada
‚îÇ   ‚îú‚îÄ‚îÄ dependes.py                            # C√≥digo librerias de spark 
|   ‚îú‚îÄ‚îÄ main.py                                # C√≥digo principal del ETL
‚îú‚îÄ‚îÄ README.md                                  # Documentaci√≥n del proyecto
```

## üîÑ Diagrama del Proceso ETL
A continuaci√≥n, se presenta un esquema visual del proceso ETL implementado:

```
    +------------+       +----------------------+       +---------------+     

    | Extracci√≥n | ----> |  Exploraci√≥n (EDA)  | ----> | Transformaci√≥n |
    +------------+       +----------------------+       +---------------+
          |                        |                         |
    Fuente de Datos       Estad√≠sticas, Valores       Limpieza de Datos  
   (hotel_bookings.csv)    Faltantes, Outliers        Creaci√≥n de Features
          |                                                  |
          |----------------------------------------------->  |
                             +---------------+ 
                             |   Carga (CSV) | 
                             +---------------+ 
                             | Almacenamiento | 
                             |   en archivos  |
                             +---------------+
```

## üì• Extracci√≥n de Datos
La extracci√≥n se realiza desde un archivo CSV almacenado en **Google Drive**. Se usa PySpark para cargar los datos:
```python
def f_extraccion(path):
    df_hotel = spark.read.csv(path, header=True, inferSchema=True)
    print(f'Columnas: {len(df_hotel.columns)}, Registros: {df_hotel.count()}')
    return df_hotel  
```

## üîç An√°lisis Exploratorio de Datos (EDA)
Se incluyen:
- Medidas de tendencia central (`df.describe()`)
- Estructura del DataFrame (`df.printSchema()`)
- An√°lisis de valores nulos y duplicados
- Detecci√≥n de outliers con diagramas de caja (Boxplot)

## üîÑ Transformaci√≥n de Datos
Las principales transformaciones incluyen:
- Conversi√≥n de tipos de datos (`IntegerType`, `DateType`)
- Tratamiento de valores nulos
- Creaci√≥n de nuevas variables (`total_huespedes`, `arrival_date_month_esp`)
- Conversi√≥n de nombres de meses a espa√±ol

Ejemplo de conversi√≥n de tipos de datos:
```python
def f_casting_df(df, num_cols, fecha_cols):
    for column in num_cols:
        df = df.withColumn(column, F.col(column).cast(IntegerType()))
    for column in fecha_cols:
        df = df.withColumn(column, to_date(df[column], 'dd-MM-yy'))
    return df
```

## üì§ Carga de Datos
Los datos transformados se guardan en archivos CSV dentro del directorio **output/**.
```python
def f_carga(df, output_path):
    df.write.mode("overwrite").option('header', 'true').csv(output_path)
```

## üèÅ Ejecuci√≥n del Pipeline ETL
Para ejecutar el pipeline, sigue estos pasos en Google Colab:

1. **Abrir Google Colab**: Ve a [Google Colab](https://colab.research.google.com/) y abre un cuaderno nuevo.

2. **Cargar Proyecto_Booking_Raul_AlvaroProleon.Zip**: En colab Ve a Archivos (lado izquierdo) y luego la opci√≥n Subir al Almacenamiento de Sesion.

3. **Descomprimir el .Zip**: (si a√∫n no est√°n instaladas):
```python
!unzip Proyecto_Booking_Raul_AlvaroProleon.zip
```
4. **Instalar Dependencias de Entorno Spark** (si a√∫n no est√°n instaladas):
```python
!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz
!tar xf spark-3.5.4-bin-hadoop3.tgz
!pip install -q findspark
```
5. **Ejecutar el script de Librerias (incluido Spark)**:
```python
!python /content/Proyecto_Booking_Raul_AlvaroProleon/src/dependes.py
```
6. **Ejecutar el script principal**: Se ejecutar√° el Pipeline de Booking con Apache Spark
```python
!python /content/Proyecto_Booking_Raul_AlvaroProleon/src/main.py
```

## üìå Autor
**Ra√∫l Alvaro Proleon**  
M√°ster en Big Data & Business Intelligence  

## üì¢ Conclusi√≥n
Este proyecto demuestra la efectividad de **Apache Spark** en el procesamiento de grandes vol√∫menes de datos mediante un pipeline **ETL** optimizado. Se logr√≥ extraer, explorar, transformar y cargar datos de reservas hoteleras, proporcionando informaci√≥n valiosa para la toma de decisiones. Gracias a la capacidad de procesamiento distribuido de Spark, el flujo de trabajo es escalable y eficiente. Este enfoque puede adaptarse para distintos dominios de an√°lisis de datos en **Big Data**.

