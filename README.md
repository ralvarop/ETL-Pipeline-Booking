# ETL Pipeline con Apache Spark en Google Colab

## ðŸ“Œ DescripciÃ³n del Proyecto
Este proyecto implementa un **ETL (Extract, Transform, Load) Pipeline** utilizando **Apache Spark** en **Google Colab**. Su objetivo es procesar datos de reservas hoteleras y generar insights clave mediante transformaciones y anÃ¡lisis exploratorio de datos (EDA).

## ðŸš€ TecnologÃ­as Utilizadas
- **Apache Spark** (PySpark)
- **Google Colab**
- **Python** (pandas, matplotlib)
- **Google Drive** (para almacenamiento de archivos)

## âš™ï¸ Requerimientos TÃ©cnicos
### ðŸ“Œ Hardware Necesario
- Procesador: MÃ­nimo **Intel Core i5** o equivalente
- Memoria RAM: **8GB** (recomendado 16GB o mÃ¡s)
- Espacio en Disco: **Al menos 10GB** libres

### ðŸ“Œ Instalaciones Preliminares
- **Google Colab** (se ejecuta en la nube, no requiere instalaciÃ³n local)
- **Google Drive** (para almacenamiento de archivos de entrada/salida)
- **Apache Spark** (instalaciÃ³n dentro de Colab, se incluye en el cÃ³digo)

### ðŸ“Œ Dependencias
Las siguientes librerÃ­as deben instalarse en el entorno de ejecuciÃ³n:
```bash
!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz
!tar xf spark-3.5.4-bin-hadoop3.tgz
!pip install -q findspark
```

## ðŸ“‚ Estructura del Proyecto
```
Proyecto_Booking_Raul_AlvaroProleon/
â”œâ”€â”€ input/                                     # Archivos de entrada
â”‚   â”œâ”€â”€ hotel_bookings.csv                     # Dataset de reservas hoteleras
â”œâ”€â”€ output/                                    # Archivos de salida
â”‚   â”œâ”€â”€ tabla_bkg_reservas.csv                 # Reservas y cancelaciones por mes y aÃ±o
â”‚   â”œâ”€â”€ tabla_bkg_dias_espera.csv              # Promedio de dÃ­as de espera y huÃ©spedes
â”‚   â”œâ”€â”€ tabla_bkg_tarifa.csv                   # Tarifas diarias promedio por habitaciÃ³n
â”œâ”€â”€ src/                                       # Archivos de entrada
â”‚   â”œâ”€â”€ dependes.py                            # CÃ³digo librerias de spark 
|   â”œâ”€â”€ main.py                                # CÃ³digo principal del ETL
â”œâ”€â”€ README.md                                  # DocumentaciÃ³n del proyecto
```

## ðŸ”„ Diagrama del Proceso ETL
A continuaciÃ³n, se presenta un esquema visual del proceso ETL implementado:

```
    +------------+       +----------------------+       +---------------+     

    | ExtracciÃ³n | ----> |  ExploraciÃ³n (EDA)  | ----> | TransformaciÃ³n |
    +------------+       +----------------------+       +---------------+
          |                        |                         |
    Fuente de Datos       EstadÃ­sticas, Valores       Limpieza de Datos  
   (hotel_bookings.csv)    Faltantes, Outliers        CreaciÃ³n de Features
          |                                                  |
          |----------------------------------------------->  |
                             +---------------+ 
                             |   Carga (CSV) | 
                             +---------------+ 
                             | Almacenamiento | 
                             |   en archivos  |
                             +---------------+
```

## ðŸ“¥ ExtracciÃ³n de Datos
La extracciÃ³n se realiza desde un archivo CSV almacenado en **Google Drive**. Se usa PySpark para cargar los datos:
```python
def f_extraccion(path):
    df_hotel = spark.read.csv(path, header=True, inferSchema=True)
    print(f'Columnas: {len(df_hotel.columns)}, Registros: {df_hotel.count()}')
    return df_hotel  
```

## ðŸ” AnÃ¡lisis Exploratorio de Datos (EDA)
Se incluyen:
- Medidas de tendencia central (`df.describe()`)
- Estructura del DataFrame (`df.printSchema()`)
- AnÃ¡lisis de valores nulos y duplicados
- DetecciÃ³n de outliers con diagramas de caja (Boxplot)

## ðŸ”„ TransformaciÃ³n de Datos
Las principales transformaciones incluyen:
- ConversiÃ³n de tipos de datos (`IntegerType`, `DateType`)
- Tratamiento de valores nulos
- CreaciÃ³n de nuevas variables (`total_huespedes`, `arrival_date_month_esp`)
- ConversiÃ³n de nombres de meses a espaÃ±ol

Ejemplo de conversiÃ³n de tipos de datos:
```python
def f_casting_df(df, num_cols, fecha_cols):
    for column in num_cols:
        df = df.withColumn(column, F.col(column).cast(IntegerType()))
    for column in fecha_cols:
        df = df.withColumn(column, to_date(df[column], 'dd-MM-yy'))
    return df
```

## ðŸ“¤ Carga de Datos
Los datos transformados se guardan en archivos CSV dentro del directorio **output/**.
```python
def f_carga(df, output_path):
    df.write.mode("overwrite").option('header', 'true').csv(output_path)
```

## ðŸ EjecuciÃ³n del Pipeline ETL
Para ejecutar el pipeline, sigue estos pasos en Google Colab:

1. **Abrir Google Colab**: Ve a [Google Colab](https://colab.research.google.com/) y abre un cuaderno nuevo.

2. **Cargar Proyecto_Booking_Raul_AlvaroProleon.Zip**: En colab Ve a Archivos (lado izquierdo) y luego la opciÃ³n Subir al Almacenamiento de Sesion.

3. **Descomprimir el .Zip**: (si aÃºn no estÃ¡n instaladas):
```python
!unzip Proyecto_Booking_Raul_AlvaroProleon.zip
```
4. **Instalar Dependencias de Entorno Spark** (si aÃºn no estÃ¡n instaladas):
```python
!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz
!tar xf spark-3.5.4-bin-hadoop3.tgz
!pip install -q findspark
```
5. **Ejecutar el script de Librerias (incluido Spark)**:
```python
!python /content/Proyecto_Booking_Raul_AlvaroProleon/src/dependes.py
```
6. **Ejecutar el script principal**: Se ejecutarÃ¡ el Pipeline de Booking con Apache Spark
```python
!python /content/Proyecto_Booking_Raul_AlvaroProleon/src/main.py
```

## ðŸ“Œ Autor
**RaÃºl Alvaro Proleon**  
Master en Big Data & Business Intelligence  

## ðŸ“œ Licencia
Este proyecto se distribuye bajo la licencia MIT.

## ðŸ“¢ ConclusiÃ³n
Este proyecto demuestra la efectividad de **Apache Spark** en el procesamiento de grandes volÃºmenes de datos mediante un pipeline **ETL** optimizado. Se logrÃ³ extraer, explorar, transformar y cargar datos de reservas hoteleras, proporcionando informaciÃ³n valiosa para la toma de decisiones. Gracias a la capacidad de procesamiento distribuido de Spark, el flujo de trabajo es escalable y eficiente. Este enfoque puede adaptarse para distintos dominios de anÃ¡lisis de datos en **Big Data**.

