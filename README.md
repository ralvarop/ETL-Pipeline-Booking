# ETL Pipeline con Apache Spark en Google Colab

## ğŸ“Œ DescripciÃ³n del Proyecto
Este proyecto implementa un **ETL (Extract, Transform, Load) Pipeline** utilizando **Apache Spark** en **Google Colab**. Su objetivo es procesar datos de reservas hoteleras y generar insights clave mediante transformaciones y anÃ¡lisis exploratorio de datos (EDA).

## ğŸš€ TecnologÃ­as Utilizadas
- **Apache Spark** (PySpark)
- **Google Colab**
- **Python** (pandas, matplotlib)
- **Google Drive** (para almacenamiento de archivos)

## âš™ï¸ Requerimientos TÃ©cnicos
### ğŸ“Œ Hardware Necesario
- Procesador: MÃ­nimo **Intel Core i5** o equivalente
- Memoria RAM: **8GB** (recomendado 16GB o mÃ¡s)
- Espacio en Disco: **Al menos 10GB** libres

### ğŸ“Œ Instalaciones Preliminares
- **Google Colab** (se ejecuta en la nube, no requiere instalaciÃ³n local)
- **Google Drive** (para almacenamiento de archivos de entrada/salida)
- **Apache Spark** (instalaciÃ³n dentro de Colab, se incluye en el cÃ³digo)

### ğŸ“Œ Dependencias
Las siguientes librerÃ­as deben instalarse en el entorno de ejecuciÃ³n:
```bash
!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz
!tar xf spark-3.5.4-bin-hadoop3.tgz
!pip install -q findspark
```

## ğŸ“‚ Estructura del Proyecto
```
Proyecto_Booking_Raul_AlvaroProleon/
â”œâ”€â”€ input/                         # Archivos de entrada
â”‚   â”œâ”€â”€ hotel_bookings.csv         # Dataset de reservas hoteleras
â”œâ”€â”€ output/                        # Archivos de salida
â”‚   â”œâ”€â”€ tabla_bkg_reservas.csv     # Reservas y cancelaciones por mes y aÃ±o
â”‚   â”œâ”€â”€ tabla_bkg_dias_espera.csv  # Promedio de dÃ­as de espera y huÃ©spedes
â”‚   â”œâ”€â”€ tabla_bkg_tarifa.csv       # Tarifas diarias promedio por habitaciÃ³n
â”œâ”€â”€ etl_pipeline.py                # CÃ³digo principal del ETL
â”œâ”€â”€ README.md                      # DocumentaciÃ³n del proyecto
```

## ğŸ“¥ ExtracciÃ³n de Datos
La extracciÃ³n se realiza desde un archivo CSV almacenado en **Google Drive**. Se usa PySpark para cargar los datos:
```python
def f_extraccion(path):
    df_hotel = spark.read.csv(path, header=True, inferSchema=True)
    print(f'Columnas: {len(df_hotel.columns)}, Registros: {df_hotel.count()}')
    return df_hotel
```

## ğŸ” AnÃ¡lisis Exploratorio de Datos (EDA)
Se incluyen:
- Medidas de tendencia central (`df.describe()`)
- Estructura del DataFrame (`df.printSchema()`)
- AnÃ¡lisis de valores nulos y duplicados
- DetecciÃ³n de outliers con diagramas de caja (Boxplot)

## ğŸ”„ TransformaciÃ³n de Datos
Las principales transformaciones incluyen:
- ConversiÃ³n de tipos de datos (`IntegerType`, `DateType`)
- Tratamiento de valores nulos
- CreaciÃ³n de nuevas variables (`total_huespedes`, `arrival_date_month_esp`)
- ConversiÃ³n de nombres de meses a espaÃ±ol

Ejemplo de conversiÃ³n de tipos de datos:
```python
def f_casting_df(df, num_cols, fecha_cols):
    for column in num_cols:
        df = df.withColumn(column, F.col(column).cast(IntegerType()))
    for column in fecha_cols:
        df = df.withColumn(column, to_date(df[column], 'dd-MM-yy'))
    return df
```

## ğŸ“¤ Carga de Datos
Los datos transformados se guardan en archivos CSV dentro del directorio **output/**.
```python
def f_carga(df, output_path):
    df.write.mode("overwrite").option('header', 'true').csv(output_path)
```

## ğŸ”„ Diagrama del Proceso ETL
A continuaciÃ³n, se presenta un esquema visual del proceso ETL implementado:

```
    +------------+       +----------------+       +---------------+
    | ExtracciÃ³n | ----> | TransformaciÃ³n | ----> | Carga (CSV)   |
    +------------+       +----------------+       +---------------+
          |                     |                        |
    Fuente de Datos       Limpieza de Datos       Almacenamiento en CSV
   (hotel_bookings.csv)   CreaciÃ³n de Features
```

## ğŸ EjecuciÃ³n del Pipeline ETL
Para ejecutar el pipeline, sigue estos pasos en Google Colab:
1. **Montar Google Drive**:
```python
from google.colab import drive
drive.mount('/content/drive')
```
2. **Instalar dependencias** (si aÃºn no estÃ¡n instaladas):
```python
!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz
!tar xf spark-3.5.4-bin-hadoop3.tgz
!pip install -q findspark
```
3. **Ejecutar el script principal**:
```python
main()
```

## ğŸ“Œ Autor
**RaÃºl Alvaro Proleon**  
Master en Big Data & Business Intelligence  

## ğŸ“œ Licencia
Este proyecto se distribuye bajo la licencia MIT.

## ğŸ“¢ ConclusiÃ³n
Este proyecto demuestra la efectividad de **Apache Spark** en el procesamiento de grandes volÃºmenes de datos mediante un pipeline **ETL** optimizado. Se logrÃ³ extraer, transformar y cargar datos de reservas hoteleras, proporcionando informaciÃ³n valiosa para la toma de decisiones. Gracias a la capacidad de procesamiento distribuido de Spark, el flujo de trabajo es escalable y eficiente. Este enfoque puede adaptarse para distintos dominios de anÃ¡lisis de datos en **Big Data**.

